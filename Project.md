## Cloud Computing Project

### Overview

The capstone project formed the final course in the Cloud Computing Specialization. It required using cloud computing technologies to process and analyse big data. The first task was to process and analyse the data using a batch processing framework and the second task was to perform the same processing and analysis, except using a stream processing framework instead. The aim was to gain experience in using real cloud computing services and big data processing tools. Doing the two tasks allowed for some comparison in the difference between the two ways of handling big data. The methods were compared, informally, mainly for their ease-of-use and performance.

## Task 1 - Batch Processing

The first task required using a **batch processing** framework to answer a series of questions about a data set pertaining to airline traffic. Amazon Web Services (AWS) was the cloud provider chosen by the course. The transport data was stored on an EC2 volume that was attached to an EC2 instance, the subset of data relating to flights was uploaded to Amazon Simple Storage (S3) bucket and then downloaded to a local machine. Downloading the data to a local machine was not required, but saved on costs by minimising time using a running EC2 instance whilst familiarising with the tools that would be needed. Apache Spark was chosen as the batch processing framework, which was used with PySpark and Jupyter Notebooks.

The data was cleaned to remove unnecessary columns, duplicate records and cancelled flights. The cleaning reduced the size of the uncompressed data from ~37GB to ~6GB, which made further handling of the data far more efficient. A Jupyter Notebook was created locally to process, query and produce the output necessary to answer the questions. The Notebook was tested on a fraction of the total data to ensure that the code worked as intended. The cleaned data was then re-uploaded to S3. The Jupyter Notebook was run on the Amazon Elastic MapReduce (EMR) service using a three instance cluster. The output consisted of console output and .csv files that were sent to S3. The .csv files were then exported from S3 to Amazon DynamoDB, where they could be easily queried.

A significant challenge during this task was finding and deciding on suitable tools to perform each task. AWS has many different services and their own platform specific terminology. A reasonable effort was spent on taking a few relevant AWS courses on Amazon's free AWS Educate platform and on reviewing relevant videos on specific services. Using Spark was relatively straight forward with the use of PySpark and Jupyter Notebooks. The Notebooks provided some interactivity, which was useful for testing queries. Familiarity with the Python Pandas library and SQL queries was immensely beneficial. Keeping costs down whilst using the AWS services was an initial concern, but the total cost for performing both the project tasks was ~$6.

[Task 1 Report](Cloud%20Computing%20Capstone%20Task%201.pdf)

## Task 2 - Stream Processing

The second task required using a **stream processing** framework to answer the same set of questions that was answered in Task 1. Spark Structured Streaming was chosen as the stream processing framework, as it offers a similar API to the one used for batch processing and allowed for easy comparison between the two. The data was already processed and cleaned from Task 1, so no further cleaning was necessary. However, the files were converted to the Apache Parquet file format for the second task. The Parquet format reduced the file size from ~6GB to ~1GB due to the compression techniques it uses and the format resulted in greater performance when tested on the PySpark queries. The stream was created by streaming from the 45 .parquet files containing the data set. The code was tested on a Jupyter Notebook as in Task 1. The Notebook was then run on an EMR cluster configured exactly as in Task 1, with the files being streamed from S3. The Task 1 queries were re-tested using the Parquet file format to gauge the performance for comparison.

Having become familiar with AWS and Spark in Task 1, the main challenge in the second task was getting the queries from Task 1 to work with Spark Structured Streaming. Spark Streaming can output the result of queries to console, files and memory, similar to regular batch processing. Spark Streaming requires that an output mode be specified as it is designed for use in scenarios where you have a potentially infinite stream of data and desire intermediate results. The output mode can be complete, append or update, which determines how results of queried are outputted as new micro-batches of data are processed. There are limitations as to which combinations of output format and output mode can be used with specific types of queries. For example, aggregation queries cannot be run in append mode unless the streamed data is time-stamped and the aggregation is using these time-stamps. In our case, the difficulty arose due to the requirement that a streaming framework be used in a scenario where we have a fixed amount of data we wish to process - the entirety of the data set within the .parquet files. Intermediate results were not required to answer the questions and would indeed produce an incorrect answer. A work-around for this problem was to write the dataframes from the queries to memory first and when no more data was being streamed from the files this was converted to a non-streaming dataframe and outputted without restrictions.

Overall the performance of the batch processing frameworks and streaming frameworks were similar when run on the cluster, but the batch processing framework completed the significantly task faster on the local machine. The comparison was slightly artificial given that the full data set was available for both methods from the start. Nevertheless, it interesting to learn to use and test both styles of big data processing on a real cloud service.

[Task 2 Report](Cloud%20Computing%20Capstone%20Task%202.pdf)
