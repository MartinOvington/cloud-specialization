# Cloud Networking

The fifth course in the specialisation focused on the networking that is necessary to enable cloud computing. It explored how networks are built to allow infrastructure to be shared, to facilitate efficient transfer of big data, to provide low latency communication and to enable applications to be used across countries and continents.

## Week 1 Traffic Patterns and Physical Structure

Almost all popular web applications run on data centre networks. These networks must run support tasks to aid in the running of these applications. This type of infrastructure is key in large scale scientific computing also. Network traffic often follows a scatter-gather pattern, where servers make several requests in response to a query and then it gathers the responses from all these requests. Nevertheless, traffic patterns within a data centre depends on the specific applications being run on it. There is a trend of growing volume of intra data centre communication, with Google noting that their traffic tends to double every year. Facebook's survey on traffic locality suggested that most communication (~58%) happens within a cluster, 18% between data centres, 13% within racks and 12% within the data centre. These statistics suggests there is not a high degree of locality of communication within a rack. Part of the reason for this type of pattern is due to the distribution of storage that is necessary for maintaining high availability.

Facebook found that web servers and 100s to 1000s of concurrent flows, whilst Hadoop nodes around 25 connections on average. Other characteristics include flow arrival times and flow sizes. These various traffic characteristics have implications for how the data centre should be designed. Some design consideration are that there is high internal traffic, there are tight deadlines for network I/O, we must consider congestion and TCP incast, there is a need for isolation across applications and there is difficulty in using centralised control at the flow level due to the high flow rates.

The modern networked applications may need high bandwidth, low latency and fault tolerance. As there are lots of small requests in data centres, the probability that there are requests with slow responses goes up, so the applications using the network may need some way of tolerating some higher latency. Applications may also experience different average latencies as the usage of the date centre network varies and the virtual machine running the application may be moved around to different parts of the network.

Planning a massive data centre requires many complex considerations such as controlling temperature and humidity. There must be careful consideration for the power infrastructure used to support the data base. Data centres are organised into servers and racks. Racks have a top of the rack switch. Data centres are organised into multiple racks that must be connected together with a network. **Intra data centre** communication is typically much greater than external communication coming in from the internet. The ideal networking structure would be to connect all racks with each other with full bandwidth connections, but this design is expensive and the switches that connect the servers together would require many ports and high throughput. The reality is that this approach limits scalability due to the limitations of the switches. An alternative approach is to use a tree structured network, where there is a hierarchy of switches. However, certain patterns of traffic that heavily utilise the root of the tree may cause congestion at these high-level switches.

Clos networks allow all racks to communicate together, but use small, cheap elements to build large, high capacity networks. The Clos network design can be used to build a **Fat-tree** network. It addresses some of the weaknesses of the traditional tree network by making it more scalable and giving it higher throughput. Fat-tree networks have a degree of redundancy in the network by having multiple routes between pairs of racks. One consideration for the design of data centre is the length of cables required. Longer cables must be optical rather than copper based, which require more expensive transceivers. Therefore, data centres converge on designs which require shorter cables. The topology of the network may affect how applications are deployed on the network, such that applications that need to communicate with each other are hosted closer together.

### Programming Assignment 1

The programming assignment required working on an emulated data centre running on a small topology consisting of two layers of switches (core switches and edge switches). The core switches connected to all edge switches and hosts were connected to the edge switches. The network was managed as a **Software Defined Network** (SDN) by having a controller connecting to all switches. The controller specified the forwarding behaviour for the switches based on a set of software defined rules. The purpose of the assignment was run this network on a virtual machine and observe the impact of a poor routing policy on a video streaming application running on the network. There was one video streaming client and one video hosting server, with the rest of the hosts running bandwidth utilising _iperf_ applications. The na√Øve routing policy sent all traffic through a single core switch, rather than using all available routes.

## Week 2 Host Virtualisation, Routing and Traffic Engineering, Congestion Control

Cloud computing relies on server virtualisation. The virtualisation used allows sharing of physical infrastructure, rapid deployment of virtual machines (VM) and live VM migration. The physical hardware the the VMs run on is managed through a hypervisor. The hypervisor manages the physical NIC (pNIC). The hypervisor runs a virtual switch, which connects to all the virtual NICs (vNICs) and the pNIC. Another way of achieving virtualisation is containers, which does not require guests OSs. Docket is an example of a container manager. Containers provide isolation between applications, but have a smaller footprint than a full guest OS on a VM. Containers can be brought up much faster than VMs.

Open vSwitch's goals are flexible and fast forwarding. The switch routing decisions are controlled from user space and packet forwarding is handled in the kernel. For the first packet of a flow, the rules in user space are consulted and subsequent packets of the flow use a collapsed rule in kernel space. Caches are used for the rules for forwarding packets, which reduces CPU utilisation at the host.

Routing is the task of finding paths between sources and destinations. Routing information needs to be install into switches in order for them to facilitate this. Individual elements of the network may not have knowledge of the entirety of the network. The Spanning Tree Protocol (STP) uses a subset of networks links to route on, with these routes representing a tree. The advantage of this protocol is that it is simple and may work well if the network is already structured in a tree-like structure. The downside is that it ignores some of the bandwidth capacity available in the network through this simplification and it does not work well in networks that aren't like trees.

Transparent Interconnection of Lots of Links (TRILL) runs a link state protocol between switches, where every link learns the topology of the network. This have the advantage over STP that all links can be utilised and multiple routes can be used. Open Shortest Path First (OSPF) over IP is another link state protocol, which also floods topology information, but uses more traditional routing and is better established. The downside of OSPF is that there is little control over routes the that are used. Border Gateway Protocol (BGP) is the standard global scale routing protocol. It gives hosts a vector of paths that represent the options that a host can make to reach the destination. BGP can be used within data centres, with the autonomous systems (AS) being groups of switches.

Multi-path routing allows for fault tolerance and traffic engineering. The Fat-tree topology is an example a network where there are multiple paths between sets of servers. Data centre operators often settle for inexact solutions for traffic engineering, as it is difficult to develop exact distributed algorithms for solving these types of problems. An example is Equal Cost Multi-Path Routing (ECMP). It works by choosing a path when represented by a choice of routes. The choice is made by a hashing of the packet header, which helps avoid the reordering of packets in a flow that would otherwise cause TCP to drop packets. ECMP essentially randomly routes based on the concept of **Flowlets**, which are packets grouped by comparing the gap between outgoing packets and the conservative estimate of the latency in network. This is more fine-grained routing than flow-level routing. CONGO is similar to ECMP, but keeps a table of congestion per-path and chooses the least congested paths for flowlets.

Congestion control is the question of how the rates of flows are controlled in the network to maximise utilisation of the available capacity, whilst avoiding unfair capacity distribution and packet loss/delays. Traditional congestion control uses end-to-end information to make decisions about congestion. This method treats the network as a black box and just uses information about whether packets are received and how long the delay was. If all packets are ACK-ed, then flow rate can be increased, otherwise flow rate can be decreased. TCP uses this type of protocol to attempt to match the networks capacity. It uses slow-start, exponential increase, additive increase and then conservative back-off. If there is a large window of packet loss, TCP disconnects and goes back to slow-start.
