# Cloud Networking

The fifth course in the specialisation focused on the networking that is necessary to enable cloud computing. It explored how networks are built to allow infrastructure to be shared, to facilitate efficient transfer of big data, to provide low latency communication and to enable applications to be used across countries and continents.

## Week 1 Traffic Patterns and Physical Structure

Almost all popular web applications run on data centre networks. These networks must run support tasks to aid in the running of these applications. This type of infrastructure is key in large scale scientific computing also. Network traffic often follows a scatter-gather pattern, where servers make several requests in response to a query and then it gathers the responses from all these requests. Nevertheless, traffic patterns within a data centre depends on the specific applications being run on it. There is a trend of growing volume of intra data centre communication, with Google noting that their traffic tends to double every year. Facebook's survey on traffic locality suggested that most communication (~58%) happens within a cluster, 18% between data centres, 13% within racks and 12% within the data centre. These statistics suggests there is not a high degree of locality of communication within a rack. Part of the reason for this type of pattern is due to the distribution of storage that is necessary for maintaining high availability.

Facebook found that web servers and 100s to 1000s of concurrent flows, whilst Hadoop nodes around 25 connections on average. Other characteristics include flow arrival times and flow sizes. These various traffic characteristics have implications for how the data centre should be designed. Some design consideration are that there is high internal traffic, there are tight deadlines for network I/O, we must consider congestion and TCP incast, there is a need for isolation across applications and there is difficulty in using centralised control at the flow level due to the high flow rates.

The modern networked applications may need high bandwidth, low latency and fault tolerance. As there are lots of small requests in data centres, the probability that there are requests with slow responses goes up, so the applications using the network may need some way of tolerating some higher latency. Applications may also experience different average latencies as the usage of the date centre network varies and the virtual machine running the application may be moved around to different parts of the network.

Planning a massive data centre requires many complex considerations such as controlling temperature and humidity. There must be careful consideration for the power infrastructure used to support the data base. Data centres are organised into servers and racks. Racks have a top of the rack switch. Data centres are organised into multiple racks that must be connected together with a network. **Intra data centre** communication is typically much greater than external communication coming in from the internet. The ideal networking structure would be to connect all racks with each other with full bandwidth connections, but this design is expensive and the switches that connect the servers together would require many ports and high throughput. The reality is that this approach limits scalability due to the limitations of the switches. An alternative approach is to use a tree structured network, where there is a hierarchy of switches. However, certain patterns of traffic that heavily utilise the root of the tree may cause congestion at these high-level switches.

Clos networks allow all racks to communicate together, but use small, cheap elements to build large, high capacity networks. The Clos network design can be used to build a **Fat-tree** network. It addresses some of the weaknesses of the traditional tree network by making it more scalable and giving it higher throughput. Fat-tree networks have a degree of redundancy in the network by having multiple routes between pairs of racks. One consideration for the design of data centre is the length of cables required. Longer cables must be optical rather than copper based, which require more expensive transceivers. Therefore, data centres converge on designs which require shorter cables. The topology of the network may affect how applications are deployed on the network, such that applications that need to communicate with each other are hosted closer together.

### Programming Assignment 1

The programming assignment required working on an emulated data centre running on a small topology consisting of two layers of switches (core switches and edge switches). The core switches connected to all edge switches and hosts were connected to the edge switches. The network was managed as a **Software Defined Network** (SDN) by having a controller connecting to all switches. The controller specified the forwarding behaviour for the switches based on a set of software defined rules. The purpose of the assignment was run this network on a virtual machine and observe the impact of a poor routing policy on a video streaming application running on the network. There was one video streaming client and one video hosting server, with the rest of the hosts running bandwidth utilising _iperf_ applications. The na√Øve routing policy sent all traffic through a single core switch, rather than using all available routes. The result was that the client detected the low available bandwidth and requested low quality video blocks from the server, which was visible on the video player.

## Week 2 Host Virtualisation, Routing and Traffic Engineering, Congestion Control

Cloud computing relies on server virtualisation. The virtualisation used allows sharing of physical infrastructure, rapid deployment of virtual machines (VM) and live VM migration. The physical hardware the the VMs run on is managed through a hypervisor. The hypervisor manages the physical NIC (pNIC). The hypervisor runs a virtual switch, which connects to all the virtual NICs (vNICs) and the pNIC. Another way of achieving virtualisation is containers, which does not require guests OSs. Docket is an example of a container manager. Containers provide isolation between applications, but have a smaller footprint than a full guest OS on a VM. Containers can be brought up much faster than VMs.

Open vSwitch's goals are flexible and fast forwarding. The switch routing decisions are controlled from user space and packet forwarding is handled in the kernel. For the first packet of a flow, the rules in user space are consulted and subsequent packets of the flow use a collapsed rule in kernel space. Caches are used for the rules for forwarding packets, which reduces CPU utilisation at the host.

Routing is the task of finding paths between sources and destinations. Routing information needs to be install into switches in order for them to facilitate this. Individual elements of the network may not have knowledge of the entirety of the network. The Spanning Tree Protocol (STP) uses a subset of networks links to route on, with these routes representing a tree. The advantage of this protocol is that it is simple and may work well if the network is already structured in a tree-like structure. The downside is that it ignores some of the bandwidth capacity available in the network through this simplification and it does not work well in networks that aren't like trees.

Transparent Interconnection of Lots of Links (TRILL) runs a link state protocol between switches, where every link learns the topology of the network. This have the advantage over STP that all links can be utilised and multiple routes can be used. Open Shortest Path First (OSPF) over IP is another link state protocol, which also floods topology information, but uses more traditional routing and is better established. The downside of OSPF is that there is little control over routes the that are used. Border Gateway Protocol (BGP) is the standard global scale routing protocol. It gives hosts a vector of paths that represent the options that a host can make to reach the destination. BGP can be used within data centres, with the autonomous systems (AS) being groups of switches.

Multi-path routing allows for fault tolerance and traffic engineering. The Fat-tree topology is an example a network where there are multiple paths between sets of servers. Data centre operators often settle for inexact solutions for traffic engineering, as it is difficult to develop exact distributed algorithms for solving these types of problems. An example is Equal Cost Multi-Path Routing (ECMP). It works by choosing a path when represented by a choice of routes. The choice is made by a hashing of the packet header, which helps avoid the reordering of packets in a flow that would otherwise cause TCP to drop packets. ECMP essentially randomly routes based on the concept of **Flowlets**, which are packets grouped by comparing the gap between outgoing packets and the conservative estimate of the latency in network. This is more fine-grained routing than flow-level routing. CONGO is similar to ECMP, but keeps a table of congestion per-path and chooses the least congested paths for flowlets.

Congestion control is the question of how the rates of flows are controlled in the network to maximise utilisation of the available capacity, whilst avoiding unfair capacity distribution and packet loss/delays. Traditional congestion control uses end-to-end information to make decisions about congestion. This method treats the network as a black box and just uses information about whether packets are received and how long the delay was. If all packets are ACK-ed, then flow rate can be increased, otherwise flow rate can be decreased. TCP uses this type of protocol to attempt to match the networks capacity. It uses slow-start, exponential increase, additive increase and then conservative back-off. If there is a large window of packet loss, TCP disconnects and goes back to slow-start.

A problem TCP are that multiplicative decrease may be too aggressive, packet loss may be a poor signal of congestion and lack of isolation across flows. Large queues in buffers can dominate the latency within a data centre. Buffers queues can cause delays to packets that do not have the same destination port. Applications using scatter-gather traffic patterns can be sensitive to latency, but can also be responsible for congestion and potentially the TCP incast problem.

High throughput and low latency is desired in the network. Data Centre TCP (DCTP) is an adaptation of TCP to the data centre environment. It aims to manage congestion before buffers start to fill up and packets are dropped. DCTP uses **Explicit Congestion Notification** (ECN). When ECN enabled switches receive packets, the switch uses the average queue length over a recent time window and then decides whether to mark the ECN bit on the packet. The switch uses a high and low threshold to decide whether to mark the ECN bit. The receiver sends back this ECN data, so the original sender can react to the congestion level. Senders can half the send rate when congestion is detected, the same as TCP, but the system helps prevent the packet being lost. However, the sender can maintain a list of ECN data that it has received and use this to proportionally reduce its send rate, rather than always halve it. DCTP can be used to reduce buffer occupancy in the network, which can be useful in reducing latency in the network. One advantage of DCTP was that it worked on existing hardware. The disadvantage is that DCTP and TCP do not work well together in the same data centre (although improved in DCTP+), DCTP still uses slow start which can be too conservative sometimes and there is no prioritising of particular flows.

## Week 3 Software-Defined Networking Architecture, Multi-Tenant Data Centres

Software-Defined Networking (SDN) attempts to solve the problem that controlling networks is otherwise complicated, control is distributed and there is no clean programming API for doing it. Controlling the network can be particularly hard in large, heterogenous networks. Innovation within traditional networks is difficult due to these characteristic. SDN uses a data plane API on the network devices in conjunction with a logically centralised controller. On top of the centralised controller are software abstractions such as a _network OS_ and on top of this there can have applications.

One of the core driving forces of SDNs, is making the network more flexible. Aspects of SDNs have been worked towards for some time. Label switching / MPLS allowed for setting explicit paths for classes of traffic. Active networks allowed packets to carry code within them that effect the operating of the network. A few projects that have historically driven towards logically centralised control including the Routing Control Platform, 4D architecture, Ethane, OpenFlow and NOX with many projects since. Hardware with standardised API make it easier for operators to change hardware and for vendors to enter the market. The centralised controller allows direct programmatic control of the network. Software abstractions make it easier to create high level policies, rather than having to work with complex distributed protocols. The challenges for SDN are scalability, distributed system challenges are still present underneath the idea of the logically centralised control, reaching agreement on the data plane protocol to use and devising the right high level abstractions. The first 'killer apps' for SDN were cloud virtualisation (for multi-tenancy) and inter-data-centre traffic engineering.

There are several key needs for building a multi-tenancy data centres. Agility is desired, which relies on location independent addressing, performance uniformity, security and network semantics. Agility is important for achieving high utilisation, because if there are otherwise lots of constraints in the data centre we may not otherwise be able to make full use of the compute resources available. Agility is also important for availability, as an application may be need to moved away from failing hardware or a highly congested portion of the network. Moving services is difficult in a traditional networking due to the lack of flexible addressing.

Case Study 1: VL2 was one of the earliest virtualised data centre designed, which influenced the design of Microsoft Azure. It was motivated by a study of data centre traffic that showed that increasing internal network traffic was a bottleneck and they found that there were unpredictable, rapidly changing traffic patterns. This resulted in the design of a non-blocking fabric i.e. network joining together servers that wasn't a bottleneck. Another motivating factor was the failure characteristics of the network, which demonstrated that it was difficult to ensure that reliability of a single device in the network. This lead to the design choice of redundancy in the core elements in the network - using a greater number of less powerful components rather than fewer, more powerful components. This is similar to a Clos network. A third motivating factor was that traffic was unpredictable, which resulted in the use of **Valiant Load balancing**, which does not rely on global coordination of traffic. The aim was to spread traffic as much as possible.

Application Addresses (AAs) are location independent and provide the illusion of a single big layer 2 switching connecting the applications. The physical network layer uses Locator Addresses (LAs) that are dependent on the location within the topology. The physical network layer uses layer 3 routing via OSPF. In between the application layer and physical network layer, there is a virtualisation layer, which contains the AA to LA mappings and a server agent responsible for wrapping AAs with an outer LA header. AAs provide some of the agility that is desired. L2 network semantics are being achieved by the agent intercepting and handling L2 broadcast and multicast. Performance uniformity is achieved through the Clos topology and the ECMP used provides load balancing. The directory system provides security by allowing or denying connections by choosing whether to resolve AAs to LAs.

Case Study 2: Network Virtualisation Platform (NVP) was introduced in a paper on network virtualisation in multi-tenant data centres. The service has an arbitrary network topology, which works on top of any standard layer 3 network. The service network and the physical network are connected through a network hypervisor. The network is modelled as Logical Datapaths, each of which has their own OpenFlow table.
