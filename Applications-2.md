## Cloud Computing Applications Part 2: Big Data and Applications

The second cloud computing applications course covered applications that are run on clouds for the processing of big data. It dealt with concerns such as storage, processing, parallelism, distribution, consensus and scalability and the key benefits and limitations of the various applications running on clouds.

### Week 1 Spark, Hortonworks, HDFS

Apache Spark is designed to be used with iterative algorithm and to offer interactive data exploration/mining, things that traditional frameworks such as Hadoop MapReduce are not good for. Hadoop requires repeated access to to HDFS and offers no optimisation to data caching and data transfers and parallelism is limited to within each iteration. Acyclic data flows are inefficient for applications that repeatedly reuse a working set of data as we require when running iterative algorithms and use interactive data mining tools. Spark aims to enhance programmability and is written on top of Scala and offers a modified interactive Scala interpreter. Multiple frameworks have been built on Spark, including Pregel (GraphX), Hive (SparkSQL) and Mllib (machine learning).

Spark runs on Apache Mesos or Yarns to share resources with Hadoop and other applications. It can read from any Hadoop input source. The Spark scheduler uses Dryad-like directed acyclic graphs (DAGs) and uses pipeline functions, cache-aware work reuse, locality awareness and partitioning awareness.

Spark uses Resilient Distributed Datasets (RDDs) to allow applications to keep working sets in memory for efficient reuse, rather than writing repeatedly to HDFS. The pieces of the RDDs can be distributed across our the nodes being used. RDDs are immutable, partitioned collections of objects that are created through parallel transformations e.g. map, filter, groupBy, join etc. and can be cached for efficient reuse. The queries done on RDDs are performed using lazy evaluations, that are only evaluated on actions e.g. count, output. RDDs achieve fault tolerance by maintaining lineage information that can be used to reconstruct lost partitions by re-performing failed computations.

HortonWorks is an example of a cloud distro, that packages together multiple connected tools. It uses HDFS and connects this to a variety of processing tools through YARN. The processing tools include MapReduce Pig, Hive etc. Users can do interactive data analysis through Apache Zeppelin and and allows for the management of Hadoop clusters through Ambari. Zeppelin can be used with Python, Spark and Scala and works in conjunction with user views generated by Tez, Hive, Pig and offers capacity scheduler views and file views (for managing, browsing and uploading to HDFS). Cloudera and MapR are alternatives to HortonWorks, that are slightly more conservative in their offerings compared to HortonWorks.

Apache Hadoop HDFS is a large scale distributed storage system. An important part of HDFS is fault tolerance, which is useful as storage devices can fail. The probability of a storage device failing increases as we scale up to having a greater number of storage devices. HDFS replicates files to achieve fault tolerance. Using HDFS allows for massive throughput that scales with the number of attached hard drives. It is optimised for reads, sequential writes and appends.

On HDFS, files are split into contiguous chunks of 16-64MB, with each chunk being replicated 2-3 times. One of the copies is placed on a different rack to improve fault tolerance. Applications communicate with HDFS through a **Name node** that has all the chunk mappings (metadata). Name nodes communicate with Data nodes that are responsible for handling the files given to them. Multiple APIs exist including Java, Python, C as well as a CLI. A HTTP browser can be used to browse files of a HDFS instance.

Mesos was built to be a scalable global resource manager for data centres and YARN was built to scale Hadoop. YARN has a scheduler, NodeManagers and ApplicationsManagers. The ResourceTrackerService handles the of membership of the system.

### Week 2 Spark MapReduce, CAP Theorem, Distributed Key-Value Store, Scalable Database, Publish-Subscribe Queues

MapReduce aims to provide a framework for users to define functions and provides automatic parallelism, fault tolerance, I/O scheduling and status monitoring.
