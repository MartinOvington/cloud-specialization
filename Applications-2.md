## Cloud Computing Applications Part 2: Big Data and Applications

The second cloud computing applications course covered applications that are run on clouds for the processing of big data. It dealt with concerns such as storage, processing, parallelism, distribution, consensus and scalability and the key benefits and limitations of the various applications running on clouds.

### Week 1 Spark, Hortonworks, HDFS

Apache Spark is designed to be used with iterative algorithm and to offer interactive data exploration/mining, things that traditional frameworks such as Hadoop MapReduce are not good for. Hadoop requires repeated access to to HDFS and offers no optimisation to data caching and data transfers and parallelism is limited to within each iteration. Acyclic data flows are inefficient for applications that repeatedly reuse a working set of data as we require when running iterative algorithms and use interactive data mining tools. Spark aims to enhance programmability and is written on top of Scala and offers a modified interactive Scala interpreter. Multiple frameworks have been built on Spark, including Pregel (GraphX), Hive (SparkSQL) and Mllib (machine learning).

Spark runs on Apache Mesos or Yarns to share resources with Hadoop and other applications. It can read from any Hadoop input source. The Spark scheduler uses Dryad-like directed acyclic graphs (DAGs) and uses pipeline functions, cache-aware work reuse, locality awareness and partitioning awareness.

Spark uses Resilient Distributed Datasets (RDDs) to allow applications to keep working sets in memory for efficient reuse, rather than writing repeatedly to HDFS. The pieces of the RDDs can be distributed across our the nodes being used. RDDs are immutable, partitioned collections of objects that are created through parallel transformations e.g. map, filter, groupBy, join etc. and can be cached for efficient reuse. The queries done on RDDs are performed using lazy evaluations, that are only evaluated on actions e.g. count, output. RDDs achieve fault tolerance by maintaining lineage information that can be used to reconstruct lost partitions by re-performing failed computations.

HortonWorks is an example of a cloud distro, that packages together multiple connected tools. It uses HDFS and connects this to a variety of processing tools through YARN. The processing tools include MapReduce Pig, Hive etc. Users can do interactive data analysis through Apache Zeppelin and and allows for the management of Hadoop clusters through Ambari. Zeppelin can be used with Python, Spark and Scala and works in conjunction with user views generated by Tez, Hive, Pig and offers capacity scheduler views and file views (for managing, browsing and uploading to HDFS). Cloudera and MapR are alternatives to HortonWorks, that are slightly more conservative in their offerings compared to HortonWorks.

Apache Hadoop HDFS is a large scale distributed storage system. An important part of HDFS is fault tolerance, which is useful as storage devices can fail. The probability of a storage device failing increases as we scale up to having a greater number of storage devices. HDFS replicates files to achieve fault tolerance. Using HDFS allows for massive throughput that scales with the number of attached hard drives. It is optimised for reads, sequential writes and appends.

On HDFS, files are split into contiguous chunks of 16-64MB, with each chunk being replicated 2-3 times. One of the copies is placed on a different rack to improve fault tolerance. Applications communicate with HDFS through a **Name node** that has all the chunk mappings (metadata). Name nodes communicate with Data nodes that are responsible for handling the files given to them. Multiple APIs exist including Java, Python, C as well as a CLI. A HTTP browser can be used to browse files of a HDFS instance.

Mesos was built to be a scalable global resource manager for data centres and YARN was built to scale Hadoop. YARN has a scheduler, NodeManagers and ApplicationsManagers. The ResourceTrackerService handles the of membership of the system.

### Week 2 Spark MapReduce, CAP Theorem, Distributed Key-Value Store, Scalable Database, Publish-Subscribe Queues

MapReduce aims to provide a framework for users to define functions and provides automatic parallelism, fault tolerance, I/O scheduling and status monitoring. It helps to solve some of the problems that would otherwise occur with Traditional Programming Models, such as deadlock, large overhead from comm. mismanagement, inability to load balance and having a framework under which it is difficult to write code. MapReduce uses distributed storage and pushes computations to where the relevant storage is. Distributed File systems is distributed storage where we have a global namespace and examples include Google GFS and Hadoop HDFS.

MapReduce is a programming model that originally comes from functional programming languages such as LISP. Many data processing problems can be phrased as a series of map and reduce functions. Map functions perform functions on key-value pairs in a dataset to create a new list of intermediate values. A reduce function takes intermediate values for a particular key and creates a set of merged output values (often just one). Example uses include word counting, Pi estimation, image smoothing, PageRank. The disadvantage of MapReduce is that there are restrictions on the set of problems that are solvable with this paradigm.

Consistency is an important concept in distributed systems. CAP theorem is the idea that you can have just two of Consistency, Availability and Partition tolerance. It is often decided that data centres should weaken consistency for faster response. Distributed computing gives rise to these problem because it operates over an unreliable network with latency and limited bandwidth, the network may not be secure, the topology may change, there may be multiple administrators, transportation has costs and the network/devices may not be homogenous. In a distributed system implemented over an unreliable network where nodes may fail, there is no way to determine whether a message has been lost or just arbitrarily delayed. There can be T-connected consistency, where system is consistent when there are no partitions, but on partitioning stale data may be returned. Once the partition heals there is then a time limit on how long before consistency is restored. Cloud services have different guaranteed properties which relate to CAP.

The **ACID** model is used for transactions and includes Atomicity (operations run once or as if once), Consistency (state is correct), Isolation (invisible concurrency), Durability (committed transactions persist). ACID is helpful as it means the developer does not need to worry about transactions ending up in a partially completed state and transactions don't see other partially completed concurrent transactions. However, using this model on the cloud can lead to unacceptable performance, partly due to poor scalability, so instead the **BASE** convention is often used. It stands for Basically Available Soft-State Services with Eventual Consistency. BASE involves executing transactions that is more concurrent and less rigid than ACID, but with weaker guarantees on consistency. Basically available means that you have a fast response even if some replicas are slow or crashed. The Soft-State Service means that you have no durable memory. The eventual consistency means that 'optimistic' answers can be sent to clients. Paxos and Zookeeper are used in BASE systems and were covered in the first two courses.

Where as Cassandra uses disk storage (covered in earlier course) Redis is an in-memory key-value store and stands for REmote DIctionary Server. Its data model is similar to a dictionary and supports not only strings, but also abstract data types such as lists of strings, sets of strings, sorted sets of string and bashes where keys and values are strings. Redis has a very fast response time as everything is in memory and it uses non-blocking I/O. Examples of use include a session store and logging.

Apache HBase is a distributed column-oriented data store built on top of HDFS and can provide storage for Hadoop Distributed Computing. Data is organised into tables, rows and columns. It provides fast record lookup, has support for record-level insertion and has supports of updates - these are not features of HDFS on its own. A table is a sparse, distributed, persistent and a multidimensional sorted map. The map is indexed using a row key, column key and a timestamp. HBase has a schema consisting of multiple tables, each table has a set of column families and the columns are dynamic columns.

Spark SQL allows for Structed Data Processing in Apache Spark and is built on top of the RDD data abstraction. Spark can read data from HDFS, Hive tables, JSON, etc. and this data can be queried using SQL. A **DataSet** is a distributed collection of data built on top of RDDs and provides the benefits of both RDDs and Spark's SQL optimised execution engine. The Datasets can be constructed from JVM objects and the acted on using functional transformations such as map, flatMap etc. and the API is available in Scala and Java (not Python due to lack of typing). **DataFrame** is a Dataset organised into named columns and can be constructed from structured data files, Hive tables, external databased or existing RDDs.

Apache Kafka is used for streaming data. It takes data from one or more producers, processes it in a Kafka cluster and then sends that out to one or more consumers. Kafka is a distributed, partitioned, replicated publish-subscribe system that provides a commit log service. It maintains feeds of messages in categories called topics and each server is called a Broker and communicates using TCP. The characteristics are that it is highly scalable, there are strong guarantees about messages (strictly ordered, persistent data) and it is distributed so that we have replication and partitioning (for fault tolerance). Each partition is replicated across a number of servers and has a leader with zero or more followers. The leaders coordinates read and write requests and ZooKeeper is used to keep servers consistent. Consumers can belong to Consumer Groups, which coordinate with each other to determine which consumer consume from which consumer.
