# Cloud Computing Specialization

## Overview

The Cloud Computing Specialization is a set of six courses, consisting of lectures, readings, challenging programming assignments and culminated in a larger project. The aim of this report is to present the key takeaways of what I've learned and to document my experience in working through the courses. The courses were my first time working with cloud / distributed computing. I have previously studied operating systems via "Operating Systems: Three Easy Pieces", networking via "Computer Networking: A-Top-Down Approach" and algorithms via "Data Structures and Algorithms in Python", all of which were very useful in working through the material and doing the assignments.

Course link: https://www.coursera.org/specializations/cloud-computing

1. [Cloud Computing Concepts Part 1](#cloud-computing-concepts-part-1)
2. [Cloud Computing Concepts Part 2](#cloud-computing-concepts-part-2)
3. Cloud Computing Applications Part 1: Cloud Systems and Infrastructure
4. Cloud Computing Applications Part 2: Big Data and Applications in the Cloud
5. Cloud Networking
6. Cloud Computing Project

## Cloud Computing Concepts Part 1

The first course in the specialization covered the internals of cloud computing - the fundamentals of distributed systems and the distributed algorithms that are used in such systems.

### Week 1 - What are Clouds, MapReduce

The market for cloud computing services has grown enormously in the last decade, with big players including Amazon AWS, Microsoft Azure and Google Cloud Computing Services. Cloud computing is a loosely defined subset of distributed systems, usually focusing on providing private or public systems that can be rented on demand to provide a variety of computing and storage services. They are often build using connecting together lots of cheaper commodity hardware, which distinguishes clouds from other distributed systems where highly specialized, expensive high performance hardware is used. The service is usually provided from one or more datacentres, with big providers having many sites throughout the world so that customers can choose ones closer to them for lower latency. The motivation is largely to provide customers with easy to use services that have low or no upfront costs, thus saving them time and money for many use cases.

The MapReduce programming paradigm introduced by Google in 2004 and consists of a series of mapping and reducing cycles used to process data. It is a flexible model that works on Key-Value pairs and can be use in large variety of cases. It is particularly well suited to distributed computing as the Map and Reduce tasks can often be parallelized due to independence of the work, thus allowing the tasks to be split across multiple workers. Workers get their data input from a distributed file system e.g. Google File System (GFS), Hadoop Distributed File System (HDFS). Intermediate results from the Map tasks are stored on workers' local file system to exploit locality and reduce tasks then use these local data as input and outputs the result to the distributed file system. Apache Hadoop is an implementation of this paradigm and consists of a Global Resource Manager (RM) that schedules tasks, Per-server Node Managers (NM) that are responsible for daemon and server specific functions and a Per-application Master that negotiates with the RM and NM and is also responsible for detecting task failures. Server failure is detecting via heartbeating and if a worker is performing a task slowly, the task is replicated and given to another worker.

### Week 2 - Gossip, Membership, Grids

Within distributed systems it is often necessary for nodes to have some knowledge of the state of other nodes within the network. Using a central node to aggregate and send out this information creates a single point of failure and may cause a bottleneck to develop in the network. The Gossip protocol, also known as Epidemic protocol is a solution to this problem that involves nodes sending out it's own aggregated state data to a randomly selected subset of the other nodes. Despite the randomness, it can be shown that such a system fairly reliably propagates data throughout the entire network and requires few cycles to do so - much like how gossip and epidemics quickly spread. Importantly, the protocol has the characteristic of fault tolerance, such that if any one node fails it usually does not constitute a total failure of the system. Cassandra, a distributed key-value store is an example of a system that uses the gossip protocol for maintaining membership lists.

A group membership list is the aggregated information as to which nodes form the system at a given point in time. In distributed systems it is important to have accurate information as to which nodes form part of the group. We must therefore both detect when a new node joins the group and when a particular node has failed and is therefore no longer part of the group. A failed node may also later recover and may need some mechanism to re-join the group. The problem of having an accurate membership list is made more difficult by the fact that nodes can fail silently and that our communication medium may drop or delay packets. We desire both **Completeness**, that each failure is in fact detected and **Accuracy**, that there is no mistaken failure detection. In reality we cannot guarantee both completeness and accuracy, so always prioritize completeness and sacrifice some accuracy. Other than gossip membership, other examples of protocol include SWIM, ring heartbeating and all-to-all heartbeating. Ring heartbeating does not work well with multiple failures and all-to-all heartbeating requires a relatively large amount of bandwidth.

Grids are used for computation-intensive computing, also known as High Performance computing (HPC). Some data processing tasks are very CPU intensive, but do require much storage or memory and such are more suited to a HPC grid solution. Scheduling for HPC consists of both intra and inter-site protocols.

### Week 3 - Peer-to-Peer (P2P) Systems, Distributed Hash Tables (DHT)

P2P systems were the first large, scalable distributed systems and some of the techniques are used in key-values stores. They are therefore very relevant to the study of cloud computing. P2P systems are split into unstructured P2P systems and structured P2P systems. They have been commonly used for file sharing, both legal and illegal, with well known platforms such as Napster and BitTorrent.

The architecture of Napster is a central set of nodes that store the file directory with clients running their own client connecting to a set of one or more central servers. When clients connect to the server, they upload the list of files that they wish to share. Clients can search the servers for desired files using keywords and gets a list of hosts for these files. The client can then directly fetch the file from the best host, usually chosen based on the available bandwidth. Clients can join the network by communicating with a well known URL, which can then introduce the client to one of the central servers.

Gnutella uses a different structure to Napster, by eliminating the need for the central servers. Gnutella is therefore a more purely distributed system and avoids the legal problems that Napster had with storing the filenames on their central servers. Instead, peers themselves can act as servers within the distributed network. There are five main message types: Query (search), QueryHit (response to query), Ping (probe network for peers), Pong (reply to ping with address of other peers) and Push (for initiating file transfer). When clients wish to search for a file, they flood peers with Query messages which are time-to-live (TTL) restricted. When a peer has the requested file, they respond with a QueryHit message, which can then begin the process of file transfer at the querying peer's discretion. The Ping/Pong messages help to deal with the high rate of churn that are typical of P2P systems, so perform the function of keeping neighbour lists fresh. The problem of this **unstructured** approach is that a high proportion of traffic is due to flooding messages in an unintelligent way.

FastTrack is a hybrid between Gnutella and Napster that takes advantage of healthier participants in the system by designating them as supernodes. Supernodes stores directory files in a subset of nearby peers. Peers then query the supernodes rather than flooding messages to all peers, which leads to faster search and less overall network traffic. Supernodes have the advantage of having the file table locally, which means that their own searches, on-average, require less network traffic.

BitTorrent is a system that incentivizes less selfish behaviour. BitTorrent maintains a tracker for file. Peers get the tracker from a known website. Trackers then give the peer information as to some of the peers that are using the required files. Peers are split into seeders who have the full file and leechers which has only some of the file. Files are split into **blocks**. Peers transfer blocks with tit-for-tat bandwidth, with preferences given to peers who transfer with higher bandwidth. Choking limits the number of neighbours to upload to the best neighbours, so uploading bandwidth is not overwhelmed.

Distributed Hash Tables (DHTs) allow for lookup similar to normal hash tables, but instead objects are in a distributed fashion over a cluster of nodes. The distributed nature introduces a new set of concerns: load-balancing, fault-tolerance, lookup/insert efficiency and locality. Chord is an example of a DHT which uses a **structured** approach, which allows for O(log(N)) memory usage, lookup latency and number of messages required for lookup. It uses consistent hashing of a peer's address, which then maps peers to one of 2^m logical points on a circle. Peers maintain a finger table, which contains the addresses of subset of peers in the circle, which allows for faster lookup of peers. Files are mapped using the same consistent hashing and stored at the peer at or the first clockwise peer after that key value on the logical circle. Failure tolerance can be achieved by replicating objects within a limited number of successors peers. Failures then require a stabilization protocol to maintain this level of replication.

Pastry is a P2P system that assigns IDs to nodes using consistent hashing like Chord, but instead uses routing tables based on prefix matching. It tries to pay attention the network topology and tries to make routes to immediate neighbours short. The greater locality is achieved naturally through the use of prefixes of the addresses. The result is that there is a shorter round-trip time than systems that do not account for the network topology, which means greater performance of this system.

Kelips builds on this idea of locality by maintaining k number of 'affinity groups', each of which is based on the locality of nodes with each other. Within an affinity group, nodes know of all other nodes within that group. One node within that group maintains a link to a node in each of the 'foreign' affinity groups. Lookup costs are then O(1), which is an improvement over Chord, but has the disadvantage of slightly worse memory usage at O(sqrt(N)).

### Week 4 - Key-Value NoSQL Stores, Time and Ordering

Key-value stores are pairs of keys of values, that allow you to lookup and retrieve values using a given key. Modern workloads has data that is often large and unstructured, has lots of random reads and writes, do no often require foreign keys, are sometimes write heavy and needs to perform joins infrequently. This means we can come up with a simpler, more suitable storage solution than tradition relational databases. Having a system where we can scale out in a cost effective fashion is desirable. NoSQL, 'Not Only SQL', helps us with many of these requirements. It is largely based on the the API consisting of get(key) and put(key, value). NoSQL tables can be structured or unstructured with no schema. NoSQL can use column based storage rather than row based storage that tradition relational database management systems (RDBMSs) use, which allow you to do column based searches faster.

Apache Cassandra is a distributed key-value store that was originally designed by Facebook. It places servers on a ring and stores keys at the server that is the successor to the key's mapping on the ring. There are primary replicas and backup replicas for all keys. There are two main replication strategies: SimpleStrategy (can be similar to Chord) and NetworkTopologyStrategy. The NetworkTopologyStrategy ensures that the second replica is stored on a different rack to the primary replica, which helps improve fault tolerance. An important consideration is **consistency** for reads and writes. Writes use an on-disk commit log for failure recovery. Writes are then applied to the memtable, which is the in-memory representation of multiple key-value pairs that can be searched by key. In the case of an unexpected shutdown, anything in the commit log is applied to the memtable. When the memtable becomes old or full, it is flushed to disk as an SSTable - the immutable data files that are used for persistent storage. SSTables can be compacted when these flushes occur and then the old, stale SSTables can be removed.

Cassandra uses **leadership election** for selecting the coordinator of requests, which is done by Apache Zookeeper (a Paxos variant). A Bloom Filter is used to efficiently check for existence of a key in the system, with some small probability of false positives. It uses bitmap in combination with a hash function to get a key's index into the bitmap, with possibilities of keys overlapping - hence the false positives.

The CAP theorem states that we can only have guarantees on two out of the three properties: Consistency (same data on all nodes), Availability (data available all the time), Partition-tolerance (system works even if network split). We must sacrifice the guarantee on the third property. Companies providing cloud services have found that availability (low latency) is often strongly correlated with profits, so this property is usually prioritised over consistency. They instead often opt for **eventual consistency** - the idea that eventually nodes will be consistent if new writes stop coming into the system. Clients then have the possibility of receiving slightly stale data, but in many use cases this is not a problem. Systems such as Cassandra can also be configured to have different consistency levels for reads/writes: quorum (threshold of nodes), one replica, all replicas, any server (not replica).

HBase is an open source system developed by Yahoo that prioritises consistency over availability. It uses a write-ahead log to help to achieve this.

In cloud systems, synchronization is required for correctness or fairness. It is a challenge because each system has their own separate clock and communication happens over the internet, which means that they have to try to achieve synchronicity within an asynchronous model. Multiple algorithms exist for synchronizing clocks in distributed systems, with different properties. Cristian's algorithm is used for external synchronization - where a system must synchronize with a clock external to the system. It calculates round-trip-times (RTT) to help to form an error interval for the actual time and then it sets the time to the middle of the interval. The Network Time Protocol (NTP) is used for synchronizing clocks on the internet, which uses a tree structure, where children sets their clocks according to their parents clock and the RTT.

Logical (or Lamport) ordering uses a happens-before relationship among pairs of events across processes. The result is a partial ordering among events within the group of processes, meaning not all events will have a causal ordering between them. We cannot distinguish between **causal** and **concurrent** events in this system. Vector clocks solve this problem by using a vector of times (one per process in system) rather than a single time. This allows us to set up rules that allow us to identify concurrent and causal events.

### Week 5 Snapshots, Multicast, Paxos

Global snapshots in clouds is used for checkpointing, garbage collection, deadlock detection and termination of computation. The global snapshot is the global state of all the processes at a given point in time. Issues related to causality, time synchronization and messages in transit make creating this snapshot challenging. The Chandy-Lamport is an algorithm for creating a global snapshot in a running system. It uses marker messages, sent out from an initiator process and then multicast to other processes on receipt of the first marker messages that it has received for this snapshot. Processes record the events and messages that happen between this first marker messages and receiving marker messages on all other incoming channels. Global snapshots should aim to create **consistent cuts**, which means that all the events that they encapsulate are causally correct i.e. if A causes B and B is in the cut, then A should be in the cut.

Within a cloud system, a **liveness** is the guarantee that something good will happen eventually e.g. a computation finishes or nodes agree on a value. **Safety** is the guarantee that nothing bad ever happens e.g. no deadlock and no conflicting consensus.

The multicast problem is where we have a piece of information at a node that needs to be sent to a group of nodes. An example of multicast is storage systems like Cassandra that use multicast to send data between nodes that are part of replica group. Sometimes we desire FIFO ordering for multicast messages. Messages sent from a given node often need to obey FIFO ordering, but it often matters less if multicast from two _different_ nodes do not obey the ordering. FIFO ordering is achieved by using a vector of sequence numbers at each process and sends a per-process sequence number with messages. Another type is causal ordering, where messages must be delivered according to the causality of events. For causal ordering, a vector of sequence numbers is sent with each message. Total ordering ensures that all receivers receive messages in the same order in all processes and achieves this using a global sequence number given by an elected leader. Reliability of multicast refers to the property that either all correct processes receive it or no processes receive it. Virtual synchrony is a technique that attempts to preserve multicast ordering and reliability in spite of failures.

The consensus problem is relevant to reliable multicasting, membership/failure detection, leader election and mutual exclusion (exclusive resource access). Consensus involves getting a group of processes to agree on a value for a given request. It is more challenging to achieve consensus in asynchronous system due to unbounded time of process execution and message delay. A large part of the problem is that within an asynchronous system we do not know for certain if another process has failed, if it simply slow or the messages are being delayed or lost.

The consensus problem has been shown to be impossible to solve in asynchronous systems. Paxos is an algorithm that tackles the consensus problem in asynchronous systems by providing a safety guarantee and eventual liveness. It is used by systems such as Zookeeper and Google's Chubby system. Paxos works in rounds with a unique ballot ID for each round. Rounds are asynchronous and are broken into phases: leader election, leader proposes a value (bill) and finally leader multicasting final value (law). The leader election requires reaching a majority (quorum). The leader then sends proposed values to all other processes, which reply OK on receipt and log the value. If the leader gets a majority of OKs, then it treats it as consensus and the value is decided.

### Programming assignment 1: Membership protocol

The programming assignment involved implementing a membership protocol. The code provided was split into an application layer, a peer-to-peer layer and an emulated network layer (EmulNet). The membership protocol was to be implemented in the P2P layer and had to satisfy completeness all the time, meaning non-faulty processes had to detect every node join, failure and leave. The protocol had to satisfy a certain level of accuracy of failure detection. The implementations of the protocol was tested under simulated message loss and single and multiple node failures.

The application layer was responsible for bootstrapping the P2P network by initializing all members of the P2P layer and having them issue join requests to the first node created. The nodes were responsible for maintaining their own membership lists and logging all joins and failures. Nodes could only communicate via the EmulNet messaging API, which had functionality for sending and receiving messages. To test the correctness of the implementation, the application layer could choose to fail nodes at will or have messages be dropped. The performance of the protocol was assessed by checking the logs that the nodes produced.

The code was written in C++ and used a mixture of C++ and C-style coding, which made it somewhat challenging to work with. The emulated network required packing in data in a way that could be reliably retrieved at the other end and I chose to use carefully aligned structs to pass in and retrieve data from the network. Each node maintained a vector for it's membership list. The membership protocol chosen was all-to-all heartbeating. SWIM and Gossip could also have been implemented and would have used less 'bandwidth', but the simplicity of all-to-all heartbeating made it attractive. A fair amount of time was spent on debugging memory issues due to peculiarities of the program using manual memory allocation, the 'new' and 'delete' keywords and implicit allocation/deallocation of memory with constructors and destructors. Using GDB and in particularly Valgrind was very useful to track down bugs. Valgrind still indicated some issues with the program at completion, but this related to code that was provided. In the end the protocol passed all tests for full marks.

## Cloud Computing Concepts Part 2

The second course continues where the first course left off, with more teaching of the internals of the cloud - concepts, techniques and industry systems.

### Week 1 Leader Election, Mutual Exclusion

Leader election is necessary in distributed systems for various reasons. From the clients perspective, they usually only wish to communicate with one of the nodes in the system, which must then act as the coordinator for the rest of the system. Another example that we have seen is where a global sequence number must be maintained, so we need a designated server for performing this function.

Within the group of nodes, there must be agreement among them as to which node is the leader. For this consensus problem, we need a leader election algorithm to decide the leader. Important considerations for the algorithm is what happens if the leader fails, who can call for an election and how we maintain safety (maximum one, best leader) and liveness (eventually a leader is elected).

The ring leader election algorithm allows any node in the ring to call an election. As the message passes around the ring, nodes with the better attributes than the previous best modifies that message with its own details and then passes it on around the ring. When a node receives its own details, it knows it is the best would-be leader and then passes on a message to inform the other nodes that it is now the leader. Within this system we require some method of failure detection such that a predecessor or successor to a failed leader can initiate a new election.

Paxos-like protocols can be used for leader election, which is what is done within Google Chubby (locking system) and Apache Zookeeper. Within Google Chubby, leaders are elected based on leaders reaching a quorum of votes. After elections, a leader receives a master lease, which guarantees that another election is not run for a set period of time. In Zookeeper, each server creates a sequence number for itself and the highest sequence number in the election messages determines the leader. It uses a two-phase commit for the leader selection which ensures that safety is maintained.

The Bully algorithm for leader election works by all processes knowing each others process IDs. When the current coordinator is detected by a given process, processes which know they are the next highest ID sends Coordinator messages to all lower IDs, which completes the election. Other processes initiate an election by sending an election message to processes that have higher IDs than itself. When a process gets an election message, it sends election messages to processes with higher IDs than itself. By detecting failures by timeouts, these election messages will eventually allow the highest ID process to detect the failure of the coordinator and send out Coordinator messages to elect itself.

Mutual exclusion is important in distributed systems as it is common for there to be multiple processes trying to read/write to the same shared resource. The problem can be broken down into the idea that within our processes we can have **critical sections** within our code that should be accessed by at most one process at a time. Within a single OS we can rely on all processes being with the same machine, which allows us to use semaphores, mutexes and condition variables. These do not work on distributed systems as they really on shared variables that we do not have in this context. Our solution for this problem requires that we have safety (max one process in critical section at a time), liveness (every request to access critical section eventually granted) and we may also desire some level of fairness by having requests be granted in FIFO ordering.

One solution to mutual exclusion in distributed systems is the central solution. We elect a leader which is responsible for maintaining a queue of requests and a special access token that allows access to the critical section. Other processes wanting to enter the critical section requests the token from the master and when it exits the critical section it must release the token back to the master.

The Ricart-Agrawala algorithm is a classical distributed systems algorithms from 1981. It allows for faster access to the critical section compared to the ring-based token passing approach. Processes multicasts requests containing lamport timestamps and then waits for all replies before entering the critical section. All processes queues outgoing replies based on the lamport timestamps and in that way fairness is maintained based on the causality inferred from the lamport timestamps.

Maekawa's algorithm is another attempt to solve the mutual exclusion problem. Access to the critical section requires replies from _some_ of the other processes, a similar to requiring a quorum. Specifically, each process is associated with a _voting set_ that it requires votes from, the optimal size of which is around the square root of the total number of nodes. It uses the intersecting voting sets to ensure mutual exclusion.

### Week 2 Concurrency and Replication Control

Remote Procedure Calls (RPCs) are an abstraction that allows a process to call a function in another process, an important concept in distributed systems. We can have different semantics for the execution of RPCs, which is needed due to the possibility of messages being dropped or processes failing. These include At-Most-Once, At-Least-Once and Maybe/Best-Effort calling semantics. **Idempotent** operations are those that can be repeated multiples times without side effects, which work well with At-Least-Once semantics. RPCs can be implemented using a stack consisting of Caller <-> Client stub <-> Caller comms module <-> Callee comms module <-> Dispatcher <-> Server stub <-> Callee. Marshalling may be required for communication between processes due to difference in architectures e.g. Big endian and Little endian. It involves using middleware that has a common data representation and converting to this format is called marshalling.

Transactions are a series of operations executed by a client that results in an RPC to a server. Transactions must either execute fully and commit or abort and have no lasting effect on the server. They must have the properties of atomicity (all or nothing principle) and isolation such that they indivisible from point of view of other transactions. We can summarise the properties of transactions using the **ACID** acronym: Atomicity, Consistency (before and after transactions), Isolation, Durability (persistence). Interleaving of transactions operating on the same data can result in lost updates (writes) and inconsistent retrieval (reads). These problems could be avoided by executing all transactions serially, but this hurts performance, so instead we wish to maximise concurrency. We use the notion of serial equivalence of interleavings of transactions if the end result is indistinguishable between interleavings. The following pairs of operations are conflicting in that their order of execution matters: read(x) & write(x), write(x) & read(x), write(x) & write(x). The following are not conflicting: read(x) & read(x), read/write(x) & read/write(y). Before committing a transaction T we check for serially equivalence with other transactions and then we can decide to abort/rollback T if serial equivalence was violated.

Pessimistic concurrency control prevents objects from accessing the same object e.g. by locking. Under this system we can allow objects to be in read mode where multiple transactions can use it or a write lock, which is exclusive. Deadlocks occur when we have two processes trying acquire two or more locks in a way that is cyclical and therefore never resolves. We can combat this using lock timeouts, but this can be inefficient. Deadlock detection can also be used to break deadlocks. However, the best way to combat deadlock is to write the system to avoid deadlocks occurring in the first place.

Optimistic concurrency control attempts to increase concurrency over pessimistic control.
One approach is to have no restrictions on write and reads, but to check for serial equivalence at the time of the commit. If serial equivalence is violated, the updates are rolled back - reads must also be aborted as they may have read 'dirty' data. Multi-version concurrency control is another approach and uses a tentative per-transaction version of objects and a committed version. Tentative versions have a timestamp and on reads and writes we find the 'correct' tentative version to read or write from. In eventual consistency systems such as Cassandra and DynamoDB, Last-Write-Wins (LWW) is used.

Replication control deals with the handling of operations when there are objects stored at multiple servers with or without replication. Fault-tolerance is achieved by increasing the number of copies of an object we have. Load-balancing is important considerations when designing system and spreading read/writes across our replicas can help to spread the load across servers. **Availability** is related to the failure probability, with more replicas we have higher availability as there is a lower probability that all replicas have failed. Two challenges arise from having replicas - there should be replication transparency (to clients it looks like only one copy) and there needs to be some level of replication consistency. A two-phase commit is used to maintain atomicity of transactions across servers holding replicas for a given transaction. The coordinator keeps logs that can be used to replay transactions in the case that a replica server crashes before replying yes/no to the transaction request. Paxos can be used to decide whether to commit a transaction and order updates.

### Week 3 Stream Processing, Distributed Graph Processing, Structure of Networks, Scheduling

Stream processing is used when we can large amounts of incoming data and we require real-time views of data e.g. twitter trends, website statistics, database intrusion detection. To achieve this we need a low latency and high-throughput system. Batch processing is not suitable for this task as we not have notions of partial results in this system and we must wait for entire computation of entire dataset to complete. Apache Storm is a widely used stream processing system. It consists of tuples streams, spouts, bolts and topologies. A spout is a source of streams, often reading from crawler or database and a bolt processes these streams into output streams. We combine spouts and bolts to from a topology. The topology can have cycles and often terminate in a single stream that forms the result. The architecture of a storm cluster is split into a master node running Nimbus, worker nodes running Supervisors and Zookeeper that coordinates the Nimbus and Supervisor communication.

Distributed graph processing can be performed using graph processing systems such as Google's Pregel system. A graph consists of a network of vertices (nodes) and edges that connect nodes. Examples of graphs are the internet graph of routers/switches, WWW graph of URL links and social graphs such as Facebook. Graph processing aims to summarise and derive properties or statistics from the graph e.g. finding shortest paths between pairs of vertices. The challenge of graph processing is that graphs can be very large, often too large to store/process on a single server. The iterative nature of graph processing makes frameworks sign as Hadoop MapReduce a poor choice. Pregel works on a cluster of servers and uses a master/work model. The master maintains a list of workers, monitors works and restarts them on failures and provides monitoring tools. The workers are responsible for processing vertices and communicating with other works. Persistent date is stored in a distributed fashion on Google File System (GFS) or BigTable and temporary data is stored on local disk. Failure-tolerance is achieved by checkpointing, failure detection and recovery.

Networks have different types of complexity. Structural complexity can be due to the sheer number of vertices and edges. Evolution can occur whereby the network changes over time e.g. people making new friends on Facebook. There can be a large amount of diversity in networks, some vertices may have many edges, some few. The vertices may represent entities that have different properties e.g. servers with different specifications. We can also have emergent phenomena where the combined simple behaviour of multiple vertices lead to complex behaviours as a result of interactions with each other. Many networks can be classified as 'small world networks' whereby any given pairs of vertices are connected by a relatively short path, even if the network is large. Clustering coefficient measures something akin to transitivity - if A-B edge exists and A-C edge exists what is the probability of C-B edge existing. Small world networks have a higher cluster coefficient and random networks have lower cluster coefficient. Small world networks have the property that if you remove random vertices, connectivity remains high, but removing a few high-degree vertices is more likely to reduce connectivity.

Schedulers should aim to achieve good throughput or response time and a high utilisation of resources. Different scheduling algorithms exist for single-processor scheduling - FIFO, Shortest Task First (STF), Round-Robin. FIFO and STF are preferable for batch applications and round-robin works well with systems that need high interactivity. Hadoop YARN has two popular schedulers - Capacity Scheduler and Hadoop Fair Scheduler. The Hadoop Capacity Scheduler uses multiple FIFO queues and guarantees each queue a set proportion of the cluster and puts higher priority tasks in queues that have more resources allocated to it. The Hadoop Fair Scheduler gives each job an equal share of resources. It divides the cluster into pools and divides resources equally between pools and prevents the cluster from being hogged by one user/job. Dominant-Resource Fair Scheduling takes into account the different proportions of the total system's resources that a task requires and attempts to fairly distribute resources according to these requirements.

### Week 4 Distributed File Systems, Distributed Shared Memory, Sensor Networks

File systems are presented to the user as a high-level abstractions that presents it as containing files and directories, which prevent users from having to think about disk blocks and memory blocks. Typical files contain a header followed by blocks containing the actual file contents. The header contains details such as timestamps, file type, ownership, access control list and reference count. Directories are simply a special case of a file and contains details of the files it contains. Distributed File Systems (DFS) stores files on a server and the client performs operations on files using RPCs. From our DFS we desire transparency for the client, functionality to support multiple concurrent client usage and replication so that we achieve fault tolerance. In DFSs we may need to authenticate the user based on some component of their messages and as they are often multi-user systems we may need some kind of authorisation e.g. through access control lists.

The Network File System (NFS) was developed by Sun Microsystems in 1980s and is widely used today. Clients and servers have virtual file systems that allows them to uses a unified API for the underlying Unix File system and NFS system. The client's NFS system communicates with the NFS system in the server, allowing clients to interact with files on the local disk of the server. Clients can mount remote files to their local directories and use the normal Unix file descriptors to work with the files. Several server optimisations exist in NFS including server caching and a choice between using delayed writes (temporarily write to memory) or write-throughs (immediate write to disk). Clients maintain caches of recently accessed blocks and a notion of **freshness** and time-modified to decide when the block needs to be refetched.

The Andrew File System (AFS) is another DFS and is mainly used in some Universities. It differs from NFS in that it works on the level of files rather than file-blocks. The design decision is based on usage patterns that show that most files are accessed by a single user and most files are small. Modern RAM capacity is large enough to cache such files and files are often consumed sequentially so having the entire file takes advantage of this locality. Reads and writes are done optimistically on local copies of files and writes are propagated to the server when the files are closed. The system uses _callback promises_ so that clients that are using a file are given notice when the server detects an update to its local copy.

Distributed Shared Memory (DSM) is the notion of processes having shared memory pages over a network. In order to achieve these we maintain a cache at each process, which processes read/write to first and we have page hits and page faults much like a local memory system. In DSM we have an owner, which is the process with the latest version of the page. Pages are in read or write states. Write state are exclusive to a single process so are mutually exclusive. Processes writing to a page multicast to other processes to invalidate their cached copies and reads are also done using multicasts if the process has no local copy. There are downsides to this approach. When two processes are both writing it can cause flip-flopping behaviour that results in a lot of network traffic. There is a need to specify page size, which may incur more overhead than is necessary for smaller read/writes, so it needs to optimised for the specific use case. We can have notions of weaker or stronger consistency within DSMs depending on how it is implemented, with strong levels of consistency resulting in slower speed.

Sensor networks have become more popular as small, higher performance (Moore's Law), low-power-usage sensors have appeared in the market. These sensors collect an enormous amount of information and sensor technology helps to filter and process this information. The sensor nodes have some way of collecting information from the environment, a microprocessor, some sort of communication link and a power source like a battery. Types of sensors include acceleration, vibration, sound, CO/CO2, pathogen detectors etc. The transmission medium is typically radio frequency, but optical transmission may be used if the available power is lower, with the downside the line of sight is needed. The sensor nodes need a small, efficient OS. The TinyOS is one such OS, which has event-driven execution and a modular, decoupled structure, which is important as we do not want unnecessary overhead using power. Often sensors will work in bursts, so that they only use power when they have to and transmit during limited windows. They will sometimes aggregate and process data to save on the power needed to transmit it.

### Week 5 Security and Datacentre Outage Studies

As distributed systems must be designed to take into account the security. Types of security threats include leakage (unauthorised access), tampering (unauthorised modification) and vandalism (interference with normal service). Common attacks including eavesdropping, masquerading, message tampering, replay attacks and denial of service (DoS). These threats are addressed by ensuring our systems protect of confidentiality, integrity and availability. **Policies** refers to the properties that we wish our security systems to have. **Mechanisms** refers to how these policies are achieved. The mechanisms usually break down into authentication, authorisation and auditing. Before designing out security system we define the attacker model, which specified the predicted capabilities of a realistic attack. Then we design and implement our security mechanisms, analyse the effectiveness of the system and measure the performance impact under normal operation.

In cryptography, a key is a sequence of bytes assigned to a user that be used to lock (encrypt) and unlock (decrypt) messages by applying the key to it. There are two main types of cryptographic systems - symmetric and asymmetric. In symmetric cryptography we must have some way of securely sharing the keys that will be used to encrypt and decrypt messages, which may be challenging as the method is vulnerable to interception of the key, so we must have some secure method of delivering the key. Conversely, asymmetric cryptography uses a **private** and **public keys**, such that messages can be encrypted with one and decrypted with the other. The public-private key technique is used for securely sending messages by encrypting the message with the recipients public key, allowing the message to be decrypted by the recipient by using their own private key. It can also be used for implementing digital signatures by encrypting items with your private key, allowing others to verify your identify by a successful decryption using the associated public key. The downside of the public/private key method is that it is more computationally expensive.

Studying datacentre outages can be useful in identifying ways in which we can improve the design of systems and procedures to prevent and better deal with future outages. Datacentre outages can be caused by power problems, over-heating, fire, DoS attacks, but by far the most common (70%) is human error. Examples of human errors include a system operator mistakenly deleting a database and its backup, a maintenance contractor shutting down an air traffic control system and a technician pulling the wrong controller and crashing a redundant storage area network that had incidentally already suffered a controller failure. Another more secure example is a technician failure to disable a fire alarm actuator prior to testing and damaging disks through the noise from the alarm.

An AWS outage occurred in 2011. Amazon published a post-mortem analysis and took steps to prevent such an outage occurring again in the future. Several companies using AWS EC2 went down, which included Reddit and Foursquare. The outage affected the Elastic Block Storage (EBS) that EC2 instances access and they are replicated for durability. The system has a secondary network that is used for overflow, but this network has a lower capacity than the primary network. The outage occurred during a routine primary network capacity upgrade, but an error occurred where traffic was routed to the secondary network, which overwhelmed it due to its lower capacity. The effect was that EBS volumes that no replica were not accessible. EBS volumes that required a replica, but now no longer had one then tried to recreate the replica, which caused even more traffic to be routed through the secondary network, thus causing even more network congestion. The Amazon team had to disable the replication requests to ease the traffic. The problem was made worse by the replication mechanism not using a **back off** protocol when initial requests fail. There was also a race condition in the EBS code that occurred only during high request rates, which caused even more nodes to fail. Amazon eventually recovered most of the data, but 0.07% of affected volumes were lost. A key lesson is that failures are often **cascading**.

Facebook suffered an outage in 2010, where Facebook was unreachable for 2.5hrs. Facebook stores data in both persistent stores and caches, which includes configuration data. Configuration values in the cache are automatically verified. The outage occurred when Facebook made a change to the persistent copy of a configuration that made it invalid. Facebook cache servers recognised the incorrect configuration and then all of them tried to fix it, which required querying a cluster of databases. The mass requests quickly overwhelmed the database, which causes failed requests. The cache severs did not use a back off protocol, making the problem even worse. Facebook disabled the website, allowing the database to recover, then slowly allowed users back on the site. After this outage a new configuration system was designed. Key lessons here are to not aggressively retry for resources and instead use a back off protocol such as exponential back off.

Planet had an outage on 2008. It was the 4th largest web hosting company and hosted 22,000 websites. The outage started with an explosion in a datacentre. A transformer caught fire, which caused an explosion of battery acid fumes the UPS backup and there was a cascading of failures. Some servers may have been moved off site to other datacentres, but there were 9,000 servers, so they could not all be moved. A key lesson is that sometimes we can lose access to data, so it is imperative to have important data be replicated, preferable across different sites.

### Programming assignment 2: Key-value store

The second programming assignment was to implement a fault-tolerant key-value store. The key-value store was to be built within the same system that was worked on in the first programming assignment, so it was required that the membership protocol would be complete and have a high degree of accuracy so that nodes could detect failures.

Requests would be made to a node that was designated as the coordinator, which would handle CRUD (create, read, update, delete) requests. Key-value pairs were to be load-balanced across the nodes. The nodes were structured in a ring and the primary copy of the key-value pair was assigned to a node based on consistent hashing of the key. To support fault tolerance, two replicas were created of each key-value pair and were placed at the successors in the ring. Requests required a quorum consistency level, meaning that two of the three requests that were sent out needed to be in agreement. After a node failure, replicas had to be recreated such that the protocol would always attempt to have three replicas.

The protocol was implemented by having nodes keep track of their last known two successors and two predecessors. They detected failures by comparing this to any updates to their overall membership list. Nodes kept track of whether they contained the primary, secondary or tertiary key-value replicas. The primary node was always responsible for ensuring that it replicated its key-value pairs to its two successors, which meant sending out messages to potentially update a tertiary replica to a secondary replica and/or create new secondary and tertiary replicas. It was therefore necessary for a node to look at it's predecessors and upgrade itself to a primary replica in the case that it was a secondary node and the primary failed or if it was the tertiary node and both the secondary and primary nodes had failed.

Both the coordinator and the servers logged the result of requests and the system was tested for its ability to respond correctly to both single and multiple node failures. It was challenging to figure out which messages had to be logged in the case of the servers sending and receiving stabilisation messages, but this was eventually resolved. Valgrind was very useful again in this assignment as a host of memory bugs required fixing before the system worked reliably.
